{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#0071BD;color:white;text-align:center;padding-top:0.8em;padding-bottom: 0.8em\">\n",
    "  LDA on Returns\n",
    "</h1>\n",
    "\n",
    "This notebook illustrates the core ideas of Latent Dirichlet Allocation on a very minimal corpus. After you have worked through this notebook, you should have understood:\n",
    "  * A __corpus__ consists of a list of documents.\n",
    "  * The __vocabulary__ consists of the union of words that we consider relevant in the documents.\n",
    "  * Each document is represented by the __word counts__ of the words in the vocabulary.\n",
    "  * A __topic__ is a probability distribution over the vocabulary.\n",
    "  * The __topic distribution__ gives us the share that each topic has on a given document.\n",
    "  * Topic distribution times topics is an approximation of the word counts.\n",
    "  \n",
    "<p style=\"background-color:#66A5D1;padding-top:0.2em;padding-bottom: 0.2em\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments in this notebook are meant as invitations to explore alternatives.\n",
    "# On the first read, you should just ignore all the comments. On a second read\n",
    "# you might want to add more sentences to the corpus (see cells below).\n",
    "# So if this is your first read, you should start ignoring comments now.\n",
    "\n",
    "# If you enlarge the corpus, you might want to enlarge the width of the notebook\n",
    "# on the screen, to see the tables without line breaks. The two lines below make\n",
    "# the cells as wide as possible:\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cell(template, value, blank, threshold):\n",
    "    print(template.format(value) if (threshold == None) or (value > threshold) else blank, end = '')\n",
    "\n",
    "def print0d(value):\n",
    "    print('( {} )'.format(value))\n",
    "\n",
    "def print1d(template, values, blank = '', threshold = None):\n",
    "    for value in values: \n",
    "        print_cell(template, value, blank, threshold)\n",
    "    print()\n",
    "    \n",
    "def print2d(template, valuess, blank = '', threshold = None):\n",
    "    for values in valuess: \n",
    "        print1d(template, values, blank, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus definition and vocabulary creation\n",
    "\n",
    "Below you find a few sentences that obviously cover two quite distinct topics. They share a common word that has two different meanings. We consider each sentence to be a separate document. Let's see whether Latend Dirichlet Allocation is able to detect that we are looking at two different topics. Notice that the process is unsupervised, i.e. **we never tell the algorithms for any document (sentence) which topic it covers**. The only hint, we will give the algorithm is that it should look out for exactly 2 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Investment in Deutsche Bank yields low return.',\n",
    "    'My investment may return nothing.', \n",
    "    'Federer’s return was good, his volley was not.',\n",
    "    'Return volley, return volley; tennis is boring.',\n",
    "    'Return on investment is on a ten year high.',\n",
    "#    'Tennis is for Federer!',\n",
    "#    'Deutsche Bank may be an investment bank.'\n",
    "]\n",
    "\n",
    "n_topics = 2\n",
    "\n",
    "expected_topics = [0, 0, 1, 1, 0] # Not for training! Just for checking afterwards.\n",
    "# expected_topics = [0, 0, 1, 1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Function_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( Words per document )\n",
      "investment  deutsche    bank        yields      return      \n",
      "investment  return      nothing     \n",
      "federer     return      good        volley      \n",
      "return      volley      return      volley      tennis      boring      \n",
      "return      investment  year        high        \n"
     ]
    }
   ],
   "source": [
    "bags = []\n",
    "\n",
    "for document in corpus:\n",
    "    \n",
    "    tokens = re.split('[ .!,;’]', document)\n",
    "    bag    = [token.lower() for token in tokens if len(token) > 3]\n",
    "    \n",
    "# Filtering words by length is just a shortcut to filtering words that do not contribute to a topic.\n",
    "# We call words that we filter out during preprocessing independently of the motivation \"stop words\". \n",
    "# Some words that hardly contribute to a topic are function words. You may instead use the following lines:\n",
    "\n",
    "#    stop_words = ['', 'in', 'my', 'may', 's', 'was', 'his', 'not', 'is', 'on', 'a', 'ten', 'for', 'be', 'an']\n",
    "#    bag        = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Some words \n",
    "#\n",
    "\n",
    "#    bag        = ['hi/lo' if word in {'high', 'low'} else word for word in bag]\n",
    "    \n",
    "    bags.append(bag)\n",
    "\n",
    "print0d('Words per document')\n",
    "print2d('{:12s}', bags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( Combined vocabulary of all documents )\n",
      "investment  deutsche  bank  yields  return  nothing  federer  good  volley  tennis  boring  year  high  \n"
     ]
    }
   ],
   "source": [
    "vocabulary = dict.fromkeys([word for bag in bags for word in bag])\n",
    "\n",
    "# We create the vocabulary from the documents by substracting\n",
    "# You could try to use a reduced vocabulary and see how LDA performs then:\n",
    "# vocabulary = dict.fromkeys(['investment', 'return', 'federer', 'volley'])\n",
    "# for bag in bags: bag = [word for word in bag if word in vocabulary]\n",
    "\n",
    "words = [word for word in vocabulary.keys()]\n",
    "\n",
    "print0d('Combined vocabulary of all documents')\n",
    "print1d('{}  ', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( Word counts in the documents )\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "investment deutsche     bank   yields   return  nothing  federer     good   volley   tennis   boring     year     high\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "        1        1        1        1        1                                                                        \n",
      "        1                                   1        1                                                               \n",
      "                                            1                 1        1        1                                    \n",
      "                                            2                                   2        1        1                  \n",
      "        1                                   1                                                              1        1\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "        3        1        1        1        6        1        1        1        3        1        1        1        1\n"
     ]
    }
   ],
   "source": [
    "for key in vocabulary.keys(): vocabulary[key] = 0\n",
    "word_counts = np.zeros((len(bags), len(words)), dtype=int)\n",
    "\n",
    "for d, bag in enumerate(bags):\n",
    "    for w, word in enumerate(words):\n",
    "        \n",
    "        count = bag.count(word)\n",
    "        \n",
    "        vocabulary[word] += count\n",
    "        word_counts[d, w] = count\n",
    "\n",
    "BLANK = 9 * ' '\n",
    "LINE = (len(vocabulary) * 9 + 5) * '~'\n",
    "print0d('Word counts in the documents'); print(LINE)\n",
    "print1d('{:>9}', vocabulary.keys()           ); print(LINE)\n",
    "print2d('{:9d}', word_counts,        BLANK, 0); print(LINE)\n",
    "print1d('{:9d}', vocabulary.values()         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( Words in the topics )\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "investment deutsche     bank   yields   return  nothing  federer     good   volley   tennis   boring     year     high\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "      3.5      1.5      1.5      1.5      3.4      1.5      0.5      0.5      0.5      0.5      0.5      1.5      1.5\n",
      "      0.5      0.5      0.5      0.5      3.6      0.5      1.5      1.5      3.5      1.5      1.5      0.5      0.5\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    19.0%     8.1%     8.1%     8.1%    18.6%     8.1%     2.7%     2.7%     2.7%     2.7%     2.7%     8.1%     8.1%\n",
      "     3.1%     3.0%     3.0%     3.0%    21.6%     3.1%     9.0%     9.0%    21.0%     9.0%     9.0%     3.1%     3.1%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components = n_topics, learning_method='batch', max_iter=50, n_jobs = -1)\n",
    "\n",
    "lda.fit(word_counts)\n",
    "\n",
    "words_in_topics = normalize(lda.components_, norm='l1')\n",
    "\n",
    "print0d('Words in the topics'); print(LINE)\n",
    "print1d('{:>9}',   vocabulary.keys()); print(LINE)\n",
    "print2d('{:9.1f}', lda.components_  ); print(LINE)\n",
    "print2d('{:9.1%}', words_in_topics  ); print(LINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( Topics in the documents )\n",
      "Topic 0  Topic 1  \n",
      "    91%           \n",
      "    85%           \n",
      "             89%  \n",
      "             92%  \n",
      "    89%           \n"
     ]
    }
   ],
   "source": [
    "topics_in_corpus = lda.transform(word_counts)\n",
    "\n",
    "print0d('Topics in the documents')\n",
    "print1d('Topic{:2d}  ', range(n_topics)             )\n",
    "print2d('{:7.0%}  ',    topics_in_corpus, BLANK, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\(^_^)/ The algorithm found the topic distribution that we expected. Nice!\n"
     ]
    }
   ],
   "source": [
    "actual_topics = np.argmax(topics_in_corpus, axis = 1)\n",
    "topics_are_as_expected = any([\n",
    "    np.array_equal(actual_topics, np.array(permutation)[expected_topics]) \n",
    "        for permutation in itertools.permutations(range(n_topics))\n",
    "])\n",
    "print('\\\\(^_^)/ The algorithm found the topic distribution that we expected. Nice!' if topics_are_as_expected \n",
    "    else '<(>_<)> NOOO!!! On this run the algorithm did NOT see the same topic distribution in the corpus as humans do.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( Actual words in documents compared to estimated words in documents )\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "investment deutsche     bank   yields   return  nothing  federer     good   volley   tennis   boring     year     high\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "        1        1        1        1        1                                                                        \n",
      "        1                                   1        1                                                               \n",
      "                                            1                 1        1        1                                    \n",
      "                                            2                                   2        1        1                  \n",
      "        1                                   1                                                              1        1\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "      0.9      0.4      0.4      0.4      0.9      0.4                                                   0.4      0.4\n",
      "      0.5                                 0.6                                                                        \n",
      "                                          0.9                                 0.8                                    \n",
      "                                          1.3               0.5      0.5      1.2      0.5      0.5                  \n",
      "      0.7                                 0.8                                                                        \n"
     ]
    }
   ],
   "source": [
    "length_in_corpus = [len(bag) for bag in bags]\n",
    "estimated_word_counts = np.diag(length_in_corpus).dot(topics_in_corpus).dot(words_in_topics)\n",
    "\n",
    "print0d('Actual words in documents compared to estimated words in documents'); print(LINE)\n",
    "print1d('{:>9}',   words                              ); print(LINE)\n",
    "print2d('{:9d}',   word_counts,           BLANK, 0    ); print(LINE)\n",
    "print2d('{:9.1f}', estimated_word_counts, BLANK, 0.334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( Short descriptions for the topics )\n",
      "investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n"
     ]
    }
   ],
   "source": [
    "def topic_description(words, probabilities):\n",
    "\n",
    "    cumulated = 0\n",
    "    description = ''\n",
    "    \n",
    "    for w in np.argsort(probabilities)[::-1]:\n",
    "\n",
    "        probability = probabilities[w]\n",
    "        description += words[w]  + ','\n",
    "        \n",
    "        if (cumulated < 1/3 <= cumulated + probability) or (cumulated < 4/5 <= cumulated + probability):\n",
    "            description += '  '\n",
    "        \n",
    "        cumulated += probability\n",
    "    \n",
    "    return description.rstrip(' ').rstrip(',')\n",
    "\n",
    "descriptions = [topic_description(words, probabilities) for probabilities in words_in_topics]\n",
    "\n",
    "print0d('Short descriptions for the topics')\n",
    "print2d('{}', descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Investment in Deutsche Bank yields low return.\"\n",
      "X  91% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-   9% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"My investment may return nothing.\"\n",
      "X  85% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  15% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Federer’s return was good, his volley was not.\"\n",
      "-  11% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "X  89% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Return volley, return volley; tennis is boring.\"\n",
      "-   8% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "X  92% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Return on investment is on a ten year high.\"\n",
      "X  89% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  11% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n"
     ]
    }
   ],
   "source": [
    "for document, probabilities in zip(corpus, topics_in_corpus):\n",
    "\n",
    "    print('\\n\"{}\"'.format(document))\n",
    "    \n",
    "    for probability, description in zip(probabilities, descriptions):\n",
    "        print('{} {:3.0%} {:}'.format('X ' if probability > 0.5 else '- ', probability, description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lattice of the \"Sentence uses word\" relation\n",
    "\n",
    "Have you been suspicious of whether we actually need a probabilistic approach to distinguish these few documents? If yes, you were right. The lattice below illustrates, which document contains which word. (A document contains a word if you can reach a word starting from the document by following lines upwards.) As you see that we could just ignore \"return\" as all documents contain this word. The presence of \"investment\" or \"volley\" separates the corpus into two. The remaining words are then just specific to each of the document.\n",
    "\n",
    "<img src='images/lda-on-returns-word-use-in-5-sentences.PNG' style='width:60%'/>\n",
    "\n",
    "So, it is time to increase the corpus a bit. Scroll back to the top and include the given two more sentences. The lattice below demonstrates that an analysis based on set theory becomes hareder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lattice of the \"Sentence uses word\" relation, given two more sentences\n",
    "<img src='images/lda-on-returns-word-use-in-7-sentences.PNG' style='width:60%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"I want to return from tennis.\"\n",
      "-  20% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "X  80% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Investment is nothing boring.\"\n",
      "X  66% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  34% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Tennis is nothing boring.\"\n",
      "-  32% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "X  68% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"After a good return, I return to volley.\"\n",
      "-  12% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "X  88% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"This is my year of high investment.\"\n",
      "X  87% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  13% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Federer's investment is at Deutsche Bank\"\n",
      "X  87% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  13% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "\"Playing tennis for hours is quite an investment.\"\n",
      "X  53% investment,return,  yields,bank,deutsche,high,year,nothing,  good,federer,volley,boring,tennis\n",
      "-  47% return,volley,  boring,tennis,good,federer,nothing,  investment,high,year,yields,bank,deutsche\n",
      "\n",
      "( Actual words in documents compared to estimated words in documents )\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "investment deutsche     bank   yields   return  nothing  federer     good   volley   tennis   boring     year     high\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "                                            1                                            1                           \n",
      "        1                                            1                                            1                  \n",
      "                                                     1                                   1        1                  \n",
      "                                            2                          1        1                                    \n",
      "        1                                                                                                  1        1\n",
      "        1        1        1                                                                                          \n",
      "        1                                                                                1                           \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "                                          0.8                                 0.7                                    \n",
      "      0.4                                 0.6                                                                        \n",
      "                                          0.6                                 0.5                                    \n",
      "                                          1.1               0.4      0.4      0.9      0.4      0.4                  \n",
      "      0.7                                 0.8                                                                        \n",
      "      0.7                                 0.8                                                                        \n",
      "      0.6                                 1.0                                 0.6                                    \n"
     ]
    }
   ],
   "source": [
    "more_documents = [\n",
    "    'I want to return from tennis.',\n",
    "    'Investment is nothing boring.',\n",
    "    'Tennis is nothing boring.',\n",
    "    'After a good return, I return to volley.',\n",
    "    'This is my year of high investment.',\n",
    "    'Federer\\'s investment is at Deutsche Bank',\n",
    "    'Playing tennis for hours is quite an investment.',\n",
    "]\n",
    "\n",
    "more_bags = [[token.lower() for token in re.split('[ .!,;’]', document) if len(token) > 3] for document in more_documents]\n",
    "more_word_counts = [[bag.count(word) for word in words] for bag in more_bags]\n",
    "\n",
    "topics_in_more_documents = lda.transform(more_word_counts)\n",
    "\n",
    "for document, probabilities in zip(more_documents, topics_in_more_documents):\n",
    "    print('\\n\"{}\"'.format(document))\n",
    "    for probability, description in zip(probabilities, descriptions):\n",
    "        print('{} {:3.0%} {:}'.format('X ' if probability > 0.5 else '- ', probability, description))     \n",
    "\n",
    "estimated_word_counts = np.diag([len(bag) for bag in more_bags]).dot(topics_in_more_documents).dot(words_in_topics)\n",
    "\n",
    "print()\n",
    "print0d('Actual words in documents compared to estimated words in documents'); print(LINE)\n",
    "print1d('{:>9}',   words                              ); print(LINE)\n",
    "print2d('{:9d}',   more_word_counts,      BLANK, 0    ); print(LINE)\n",
    "print2d('{:9.1f}', estimated_word_counts, BLANK, 0.334)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"1\" style=\"text-align:left;background-color:#0071BD;color:white\">\n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">\n",
    "            <img alt=\"Creative Commons License\" style=\"border-width:0;float:left;padding-right:10pt\"\n",
    "                 src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" />\n",
    "        </a>\n",
    "        &copy; D. Speicher<br/>\n",
    "        Licensed under a \n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\" style=\"color:white\">\n",
    "            CC BY-NC 4.0\n",
    "        </a>.\n",
    "      </td>\n",
    "      <td colspan=\"2\" style=\"text-align:left;background-color:#66A5D1\">\n",
    "          <b>Acknowledgments:</b>\n",
    "          This material was prepared within the project\n",
    "          <a href=\"http://www.b-it-center.de/b-it-programmes/teaching-material/p3ml/\" style=\"color:black\">\n",
    "              P3ML\n",
    "          </a> \n",
    "          which is funded by the Ministry of Education and Research of Germany (BMBF)\n",
    "          under grant number 01/S17064. The authors gratefully acknowledge this support.\n",
    "      </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
