{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#0071BD;color:white;text-align:center;padding-top:0.8em;padding-bottom: 0.8em\">\n",
    "  LDA Spike 2 - Counting\n",
    "</h1>\n",
    "\n",
    "This notebook counts the occurrences of words in the cleaned the text files. By default the cleaned text files are expected to be found in the folder `Cleaned` and the count files are written into the folder `Counts`. We leave the counting to [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from `sklearn.feature_extraction.text`. The most time is spent for separating the matrix of all counts and storing the counts for each file separately. We invest this time so that the counts may easily be reviewed manually.\n",
    "\n",
    "<font color=\"darkred\">__This notebooks writes to and reads from your file system.__ Per default all used directory are within `~/TextData/Abgeordnetenwatch`, where `~` stands for whatever your operating system considers your home directory. To change this configuration either change the default values in the second next cell or edit [LDA Spike - Configuration.ipynb](./LDA%20Spike%20-%20Configuration.ipynb) and run it before you run this notebook.</font>\n",
    "\n",
    "This notebooks operates on text files. In our case we retrieved these texts from www.abgeordnetenwatch.de guided by data that was made available under the [Open Database License (ODbL) v1.0](https://opendatacommons.org/licenses/odbl/1.0/).\n",
    "\n",
    "<p style=\"background-color:#66A5D1;padding-top:0.2em;padding-bottom: 0.2em\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stored values of configuration parameters or set a default\n",
    "\n",
    "%store -r project_name\n",
    "if not('project_name' in globals()): project_name = 'AbgeordnetenWatch'\n",
    "\n",
    "%store -r text_data_dir\n",
    "if not('text_data_dir' in globals()): text_data_dir = Path.home() / 'TextData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_only_missing_counts = True\n",
    "\n",
    "cleaned_dir = text_data_dir / project_name / 'Cleaned'\n",
    "counts_dir  = text_data_dir / project_name / 'Counts'\n",
    "\n",
    "assert cleaned_dir.exists(),                      'Directory should exist.'\n",
    "assert cleaned_dir.is_dir(),                      'Directory should be a directory.'\n",
    "assert next(cleaned_dir.iterdir(), None) != None, 'Directory should not be empty.'\n",
    "\n",
    "counts_dir.mkdir(parents=True, exist_ok=True) # Creates a local directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 7767 documents: \"achim-kessler_die-linke_Q0001_2017-08-06_A01_2017-08-11_gesundheit.txt\" ... \"zaklin-nastic_die-linke_Q0008_2017-10-25_A01_2018-09-24_demokratie-und-bürgerrechte.txt\"\"\n"
     ]
    }
   ],
   "source": [
    "filenames = []\n",
    "texts = []\n",
    "\n",
    "files = list(cleaned_dir.glob('*A*.txt')) # Answers\n",
    "list.sort(files)\n",
    "\n",
    "for file in files:\n",
    "    filenames.append(file.name)\n",
    "    texts.append(file.read_text())\n",
    "    \n",
    "print('Read {} documents: \"{}\" ... \"{}\"\"'.format(len(filenames), filenames[0], filenames[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted 9568 unique words.\n",
      "Counting took 0.85s.\n"
     ]
    }
   ],
   "source": [
    "# See: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\n",
    "counter_start_time = time.perf_counter()\n",
    "\n",
    "counter = CountVectorizer(analyzer='word', min_df=8, max_df = 0.80, lowercase=False)\n",
    "\n",
    "word_counts = counter.fit_transform(texts)\n",
    "words       = counter.get_feature_names()\n",
    "\n",
    "print('Counted {} unique words.'.format(len(words)))\n",
    "\n",
    "counter_end_time = time.perf_counter()\n",
    "print('Counting took {:.2f}s.'.format(counter_end_time - counter_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping the word counts to files took 1.82s.\n"
     ]
    }
   ],
   "source": [
    "dump_start_time = time.perf_counter()\n",
    "\n",
    "for doc, filename in enumerate(filenames):\n",
    "\n",
    "    target_file = counts_dir / (filename + '.count')\n",
    "    if update_only_missing_counts and target_file.exists(): continue\n",
    "\n",
    "    counts = {}\n",
    "    doc_word_counts = word_counts[doc, :]\n",
    "    _, word_indices = word_counts[doc, :].nonzero()\n",
    "\n",
    "    for word in word_indices:\n",
    "        counts[words[word]] = str(doc_word_counts[0, word])\n",
    "\n",
    "    target_file.write_text(json.dumps(counts, ensure_ascii=False, indent=0, sort_keys=True))\n",
    "\n",
    "dump_end_time = time.perf_counter()\n",
    "print('Dumping the word counts to files took {:.2f}s.'.format(dump_end_time - dump_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five most frequent words for some random documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felix-schreiner_cdu_Q0006_2018-0:    4 \"gelten\"       2 \"Jahr\"         2 \"gewährleist   2 \"Baden\"        2 \"breit\"     \n",
      "martin-sichert_afd_Q0006_2017-09:    3 \"Geld\"         3 \"wollen\"       3 \"Steuerzahle   3 \"Hartz\"        3 \"Million\"   \n",
      "annalena-baerbock_die-grünen_Q00:    4 \"Ukraine\"      2 \"Russland\"     2 \"Abkomme\"      2 \"Deutschland   2 \"russisch\"  \n",
      "heike-baehrens_spd_Q0002_2017-08:    7 \"Waffe\"        5 \"illegal\"      3 \"Waffenrecht   3 \"begegnen\"     3 \"wollen\"    \n",
      "annalena-baerbock_die-grünen_Q00:    3 \"Werbung\"      2 \"verbieten\"    2 \"Gesetzentwu   2 \"Bundestag\"    2 \"Satz\"      \n",
      "michaela-noll_cdu_Q0003_2017-09-:    9 \"Afghanistan   8 \"Mensch\"       6 \"Land\"         5 \"Rückführung   5 \"können\"    \n",
      "hubertus-heil_spd_Q0032_2018-08-:    9 \"Deutschland   6 \"Rente\"        6 \"Österreich\"   5 \"Alterssiche   4 \"demografisc\n"
     ]
    }
   ],
   "source": [
    "# For slice the notation [from:to:step] see the\n",
    "# reference https://docs.python.org/3/library/stdtypes.html?highlight=slice%20notation#common-sequence-operations or the\n",
    "# explanation https://stackoverflow.com/questions/509211/understanding-pythons-slice-notation/509295#509295\n",
    "\n",
    "# For sorting with argsort see\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n",
    "# https://docs.scipy.org/doc/numpy/reference/routines.sort.html\n",
    "\n",
    "import random as rnd\n",
    "\n",
    "for _ in range(7):\n",
    "    \n",
    "    doc = rnd.randint(0, len(filenames))\n",
    "    filename = filenames[doc]\n",
    "    \n",
    "    print('{:32.32}: '.format(filename), end ='')\n",
    "    \n",
    "    word_count    = word_counts[doc, :].toarray().flatten()\n",
    "    most_frequent = np.argsort(word_count)[:-6:-1]\n",
    "    \n",
    "    for word in most_frequent:\n",
    "        print('{:4} {:12.12}'.format(word_counts[doc, word], '\"' + words[word] + '\"'), end = '')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Runtime of the notebook \n",
      "-------------------------\n",
      "    0.85s  Counting the words\n",
      "    1.82s  Dumping the word counts to files\n",
      "    5.18s  All calculations together\n"
     ]
    }
   ],
   "source": [
    "notebook_end_time = time.perf_counter()\n",
    "\n",
    "print()\n",
    "print(' Runtime of the notebook ')\n",
    "print('-------------------------')\n",
    "print('{:8.2f}s  Counting the words'.format(\n",
    "    counter_end_time - counter_start_time))\n",
    "print('{:8.2f}s  Dumping the word counts to files'.format(\n",
    "    dump_end_time - dump_start_time))\n",
    "print('{:8.2f}s  All calculations together'.format(\n",
    "    notebook_end_time - notebook_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"1\" style=\"text-align:left;background-color:#0071BD;color:white\">\n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">\n",
    "            <img alt=\"Creative Commons License\" style=\"border-width:0;float:left;padding-right:10pt\"\n",
    "                 src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" />\n",
    "        </a>\n",
    "        &copy; D. Speicher, T. Dong<br/>\n",
    "        Licensed under a \n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\" style=\"color:white\">\n",
    "            CC BY-NC 4.0\n",
    "        </a>.\n",
    "      </td>\n",
    "      <td colspan=\"2\" style=\"text-align:left;background-color:#66A5D1\">\n",
    "          <b>Acknowledgments:</b>\n",
    "          This material was prepared within the project\n",
    "          <a href=\"http://www.b-it-center.de/b-it-programmes/teaching-material/p3ml/\" style=\"color:black\">\n",
    "              P3ML\n",
    "          </a> \n",
    "          which is funded by the Ministry of Education and Research of Germany (BMBF)\n",
    "          under grant number 01/S17064. The authors gratefully acknowledge this support.\n",
    "      </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
