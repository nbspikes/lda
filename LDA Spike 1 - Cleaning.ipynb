{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#0071BD;color:white;text-align:center;padding-top:0.8em;padding-bottom: 0.8em\">\n",
    "  LDA Spike 1 - Cleaning\n",
    "</h1>\n",
    "\n",
    "This notebook \"cleans\" the text files containing answers with the help of the Natural Language Processing Library [spaCy](https://spacy.io/). By default the text files are expected to be found in the folder `Corpus` and the cleaned files are written into the folder `Cleaned`. We want to keep only useful information in the files and remove any \"noise\". We decided to do the following:\n",
    "\n",
    "  * Replace all words by their lemmata ('sang', 'singe', 'singt' --> 'singen').\n",
    "  * Keep the capitalization for nouns and proper nouns but otherwise change to lower case.\n",
    "  * Keep only verbs, nouns, proper nouns and adjectives.\n",
    "\n",
    "The randomly picked example below will (probably) demonstrate the impact of these transformations. Nevertheless, there is still much room for improvement. You may try other NLP libraries as well or even skip this step altogether.\n",
    "\n",
    "<font color=\"darkred\">__This notebooks writes to and reads from your file system.__ Per default all used directory are within `~/TextData/Abgeordnetenwatch`, where `~` stands for whatever your operating system considers your home directory. To change this configuration either change the default values in the second next cell or edit [LDA Spike - Configuration.ipynb](./LDA%20Spike%20-%20Configuration.ipynb) and run it before you run this notebook.</font>\n",
    "\n",
    "This notebooks operates on text files. In our case we retrieved these texts from www.abgeordnetenwatch.de guided by data that was made available under the [Open Database License (ODbL) v1.0](https://opendatacommons.org/licenses/odbl/1.0/).\n",
    "\n",
    "<p style=\"background-color:#66A5D1;padding-top:0.2em;padding-bottom: 0.2em\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import random as rnd\n",
    "import time\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stored values of configuration parameters or set a default\n",
    "\n",
    "%store -r project_name\n",
    "if not('project_name' in globals()): project_name = 'AbgeordnetenWatch'\n",
    "\n",
    "%store -r text_data_dir\n",
    "if not('text_data_dir' in globals()): text_data_dir = Path.home() / 'TextData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_only_missing_texts = True\n",
    "\n",
    "corpus_dir  = text_data_dir / project_name / 'Corpus'\n",
    "cleaned_dir = text_data_dir / project_name / 'Cleaned'\n",
    "\n",
    "assert corpus_dir.exists(),                      'Directory should exist.'\n",
    "assert corpus_dir.is_dir(),                      'Directory should be a directory.'\n",
    "assert next(corpus_dir.iterdir(), None) != None, 'Directory should not be empty.'\n",
    "\n",
    "cleaned_dir.mkdir(parents=True, exist_ok=True) # Creates a local directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Configuration and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notaword_pos = ['SPACE', 'PUNCT']\n",
    "keepcase_pos = ['NOUN', 'PROPN']\n",
    "keepword_pos = ['ADJ', 'NOUN', 'PROPN', 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "german = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_text(text):\n",
    "    text_model = german(text)\n",
    "    lemmata = [token.lemma_ if token.pos_ in keepcase_pos else token.lemma_.lower() \n",
    "                   for token in text_model if token.pos_ in keepword_pos]\n",
    "    return ' '.join(lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Kuh rannte bis sie fiel, in die Vertiefung. --> Kuh rennen fallen Vertiefung\n"
     ]
    }
   ],
   "source": [
    "text = 'Die Kuh rannte bis sie fiel, in die Vertiefung.'\n",
    "print(text, '-->', cleaned_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_filenames = []\n",
    "answer_texts = []\n",
    "\n",
    "files = list(corpus_dir.glob('*A*.txt'))\n",
    "list.sort(files)\n",
    "\n",
    "for file in files:\n",
    "    answer_filenames.append(file.name)\n",
    "    answer_texts.append(file.read_text().strip())\n",
    "\n",
    "files = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Example Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr-gregor-gysi_die-linke_Q0053_2018-05-06_A01_2018-05-22_demokratie-und-bürgerrechte.txt\n",
      "\n",
      "Sehr geehrte Frau  N.N. ,\n",
      "an den elf Tagen, an denen ich nicht im Bundestag war, war ich keineswegs krank. Ich fehlte deshalb entschuldigt, weil ich Mandatspflichten, also Pflichten als Bundestagsabgeordneter außerhalb Berlins wahrnahm. So sprach ich zum Beispiel vor Gewerkschaftern, an Universitäten oder vor Unternehmern.\n",
      "Mit freundlichen Grüßen\n",
      "Gregor Gysi\n"
     ]
    }
   ],
   "source": [
    "min_len = 300\n",
    "max_len = 600\n",
    "example_text = ''\n",
    "\n",
    "while (len(example_text) < min_len or len(example_text) > max_len):\n",
    "    example = rnd.randint(0, len(answer_filenames))\n",
    "    example_text = answer_texts[example]\n",
    "\n",
    "print(answer_filenames[example])\n",
    "print()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model of the text. We use POS-Tagging to filter the words:\n",
    "# https://spacy.io/api/annotation#pos-tagging\n",
    "\n",
    "text_model = german(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatized words with part of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sehr ADV geehrt ADJ Frau NOUN  N.N. PROPN ,\n",
      "an ADP der DET elf NUM Tag NOUN ,an ADP der PRON ich PRON nicht PART im ADP Bundestag NOUN sein AUX ,sein AUX ich PRON keineswegs ADV kranken ADJ .Ich PRON fehlen VERB deshalb ADV entschuldigen VERB ,weil SCONJ ich PRON Mandatspflichten NOUN ,also ADV Pflicht NOUN als ADP Bundestagsabgeordneter NOUN außerhalb ADP Berlin PROPN wahrnehmen VERB .So ADV sprechen VERB ich PRON zum ADP Beispiel NOUN vor ADP Gewerkschafter NOUN ,an ADP Universität NOUN oder CONJ vor ADP Unternehmer NOUN .\n",
      "Mit ADP freundlich ADJ Gruß NOUN \n",
      "Gregor PROPN Gysi PROPN "
     ]
    }
   ],
   "source": [
    "for token in text_model:\n",
    "    if token.pos_ in notaword_pos: \n",
    "        print(token, end='') \n",
    "    else: \n",
    "        print(token.lemma_, token.pos_, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words by part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ  : freundlichen, geehrte, krank\n",
      "ADP  : als, an, außerhalb, im, mit, vor, zum\n",
      "ADV  : also, deshalb, keineswegs, sehr, so\n",
      "AUX  : war\n",
      "CONJ : oder\n",
      "DET  : den\n",
      "NOUN : Beispiel, Bundestag, Bundestagsabgeordneter, Frau, Gewerkschaftern, Grüßen, Mandatspflichten, Pflichten, Tagen, Universitäten, Unternehmern\n",
      "NUM  : elf\n",
      "PART : nicht\n",
      "PRON : denen, ich\n",
      "PROPN: Berlins, Gregor, Gysi, N.N.\n",
      "SCONJ: weil\n",
      "VERB : entschuldigt, fehlte, sprach, wahrnahm\n"
     ]
    }
   ],
   "source": [
    "parts_of_speech = {}\n",
    "\n",
    "for token in text_model:\n",
    "    pos = token.pos_\n",
    "    if pos in ['SPACE', 'PUNCT']: continue\n",
    "    words = parts_of_speech.setdefault(pos, set())\n",
    "    if pos in keepcase_pos: words.add(token.text)\n",
    "    else: words.add(token.text.lower())\n",
    "\n",
    "for key in sorted(parts_of_speech.keys()):\n",
    "    words = list(parts_of_speech[key])\n",
    "    list.sort(words)\n",
    "    print('{:5}: {}'.format(key, ', '.join(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlins -> Berlin, Gewerkschaftern -> Gewerkschafter, Grüßen -> Gruß, Pflichten -> Pflicht, Tagen -> Tag, Universitäten -> Universität, Unternehmern -> Unternehmer, den -> der, denen -> der, entschuldigt -> entschuldigen, fehlte -> fehlen, freundlichen -> freundlich, geehrte -> geehrt, krank -> kranken, sprach -> sprechen, wahrnahm -> wahrnehmen, war -> sein\n"
     ]
    }
   ],
   "source": [
    "lemmatizations = list(set(\n",
    "    token.text + ' -> ' + token.lemma_ \n",
    "    for token in text_model if token.text != token.lemma_\n",
    "))\n",
    "list.sort(lemmatizations)\n",
    "print(', '.join(lemmatizations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered by part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geehrt Frau N.N. Tag Bundestag kranken fehlen entschuldigen Mandatspflichten Pflicht Bundestagsabgeordneter Berlin wahrnehmen sprechen Beispiel Gewerkschafter Universität Unternehmer freundlich Gruß Gregor Gysi "
     ]
    }
   ],
   "source": [
    "for token in text_model:\n",
    "    if token.pos_ in keepword_pos: \n",
    "        print(token.lemma_, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Example Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Original text: ------------------------------\n",
      "Sehr geehrte Frau  N.N. ,\n",
      "an den elf Tagen, an denen ich nicht im Bundestag war, war ich keineswegs krank. Ich fehlte deshalb entschuldigt, weil ich Mandatspflichten, also Pflichten als Bundestagsabgeordneter außerhalb Berlins wahrnahm. So sprach ich zum Beispiel vor Gewerkschaftern, an Universitäten oder vor Unternehmern.\n",
      "Mit freundlichen Grüßen\n",
      "Gregor Gysi\n",
      "------------------------------ Cleaned text: ------------------------------\n",
      "geehrt Frau N.N. Tag Bundestag kranken fehlen entschuldigen Mandatspflichten Pflicht Bundestagsabgeordneter Berlin wahrnehmen sprechen Beispiel Gewerkschafter Universität Unternehmer freundlich Gruß Gregor Gysi\n"
     ]
    }
   ],
   "source": [
    "print(30 * '-' + ' Original text: ' + 30 * '-')\n",
    "print(example_text)\n",
    "print(30 * '-' + ' Cleaned text: ' + 30 * '-')\n",
    "print(cleaned_text(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write all cleaned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing the text as natural language and cleaning took 1.84s\n"
     ]
    }
   ],
   "source": [
    "nlp_start_time = time.perf_counter()\n",
    "\n",
    "success = []\n",
    "failure = []\n",
    "   \n",
    "for filename, answer_text in zip(answer_filenames, answer_texts):\n",
    "\n",
    "    target_file = cleaned_dir / filename\n",
    "    if update_only_missing_texts and target_file.exists(): continue\n",
    "        \n",
    "    try:\n",
    "        target_file.write_text(cleaned_text(answer_text))\n",
    "        success.append(filename)\n",
    "\n",
    "    except Exception as exception:\n",
    "        failure.append((filename, exception))\n",
    "\n",
    "    finally:\n",
    "        print('\\r{} files succesfully processed. {} files failed.'.format(len(success), len(failure)), end='')\n",
    "\n",
    "nlp_end_time = time.perf_counter()\n",
    "print('\\nParsing the text as natural language and cleaning took {:.2f}s'.format(nlp_end_time - nlp_start_time))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No exception during preprocessing :-)\n"
     ]
    }
   ],
   "source": [
    "for filename, exception in failure:\n",
    "    print('Exception while processing \"{}\" was:'.format(filename))\n",
    "    print(exception)\n",
    "else:\n",
    "    print('No exception during preprocessing :-)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"1\" style=\"text-align:left;background-color:#0071BD;color:white\">\n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">\n",
    "            <img alt=\"Creative Commons License\" style=\"border-width:0;float:left;padding-right:10pt\"\n",
    "                 src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" />\n",
    "        </a>\n",
    "        &copy; T. Dong, D. Speicher<br/>\n",
    "        Licensed under a \n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\" style=\"color:white\">\n",
    "            CC BY-NC 4.0\n",
    "        </a>.\n",
    "      </td>\n",
    "      <td colspan=\"2\" style=\"text-align:left;background-color:#66A5D1\">\n",
    "          <b>Acknowledgments:</b>\n",
    "          This material was prepared within the project\n",
    "          <a href=\"http://www.b-it-center.de/b-it-programmes/teaching-material/p3ml/\" style=\"color:black\">\n",
    "              P3ML\n",
    "          </a> \n",
    "          which is funded by the Ministry of Education and Research of Germany (BMBF)\n",
    "          under grant number 01/S17064. The authors gratefully acknowledge this support.\n",
    "      </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
